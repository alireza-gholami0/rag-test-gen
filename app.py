import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEndpoint
from langchain_huggingface import ChatHuggingFace
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
import tempfile
from dotenv import load_dotenv
import os

load_dotenv()

INDEX_PATH = os.getenv("INDEX_PATH", "faiss_index")
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_MODEL_REPO = os.getenv("HF_MODEL_REPO", "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B")
HF_MAX_TOKENS = int(os.getenv("HF_MAX_TOKENS", 512))
HF_TEMPERATURE = float(os.getenv("HF_TEMPERATURE", 0.0))

# --- 1. Index documents ---
def index_documents(pdf_files):
    docs = []
    for uploaded_file in pdf_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            loader = PyPDFLoader(tmp_file.name)
            docs += loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
    chunks = splitter.split_documents(docs)

    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectordb = FAISS.from_documents(chunks, embedder)
    vectordb.save_local(INDEX_PATH)
    return vectordb, embedder

# --- 2. Create prompt ---
def create_prompt(requirements, ts_description, example_ts=None, example_desc=None):
    example_part = ""
    if example_ts and example_desc:
        example_part = f"""
            Ù†Ù…ÙˆÙ†Ù‡ ØªÙˆØ¶ÛŒØ­ ÛŒÚ© Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª:
        {example_ts}
        Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„:
        {example_desc}
        """

    base_prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± Ø³ÛŒØ³ØªÙ… Ù‡Ø³ØªÛŒØ¯ Ùˆ ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ø±ÛŒØ¯ ÛŒÚ© Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯. 
    Ù‡Ø¯Ù Ø§ÛŒÙ† Ø³Ù†Ø§Ø±ÛŒÙˆ Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø³Øª. 
    Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª Ø¨Ø§ ØªÙˆØ¶ÛŒØ­ Ú¯Ø§Ù…â€ŒØ¨Ù‡â€ŒÚ¯Ø§Ù… ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ú†Ú¯ÙˆÙ†Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ´ÙˆØ¯:
    ØªÙˆØ¶ÛŒØ­ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±:
    {ts_description}
    Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡:
    {requirements}
    {example_part}
    Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒØŒ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª Ø¨Ø§ Ù…Ø±Ø§Ø­Ù„ Ú©Ø§Ù…Ù„ØŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ ÛŒØ§ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø³Ù†Ø§Ø±ÛŒÙˆ Ú†Ú¯ÙˆÙ†Ù‡ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ùˆ ØªÙˆØ¶ÛŒØ­ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
    """
    return base_prompt

# --- 3. Generate test scenario ---
def generate_test_scenario(prompt, vectordb):
    retriever = vectordb.as_retriever(search_kwargs={"k": 1})
    docs = retriever.invoke(prompt)
    context = "\n\n".join([d.page_content for d in docs])
    final_prompt = f"""
    context:
    {context}

    question:
    {prompt}
    """

    llm_endpoint = HuggingFaceEndpoint(
        repo_id=HF_MODEL_REPO,
        task="text2text-generation",
        huggingfacehub_api_token=HF_API_TOKEN,
        temperature=HF_TEMPERATURE,
        max_new_tokens=HF_MAX_TOKENS
    )
    chat_model = ChatHuggingFace(llm=llm_endpoint)

    try:
        return chat_model.invoke(final_prompt)
    except Exception as e:
        return f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„: {e}"
    
# --- 4. Streamlit UI ---
st.set_page_config(page_title="ØªÙˆÙ„ÛŒØ¯ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªØ³Øª", layout="wide")
st.markdown(
    """
    <style>
    html, body, .main, [data-testid="stAppViewContainer"] {
        direction: rtl !important;
        text-align: right !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ§ªTest Scenario Generation")

uploaded_pdfs = st.file_uploader(
    "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF Ø¯Ø§Ù…Ù†Ù‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯",
    type="pdf",
    accept_multiple_files=True
)

if st.button("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ/Ø§ÛŒÙ†Ø¯Ú©Ø³ PDF"):
    if uploaded_pdfs:
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDFÙ‡Ø§..."):
            embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            vectordb, embedder = index_documents(uploaded_pdfs)
            st.success("Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    else:
        st.warning("Ù„Ø·ÙØ§Ù‹ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© PDF Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.")


requirements = st.text_area("Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§")
description = st.text_area("ØªÙˆØ¶ÛŒØ­ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±")
example_desc = st.text_area("Ø´Ø±Ø­ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")
example_steps = st.text_area("Ù…Ø±Ø§Ø­Ù„ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)")



if st.button("ØªÙˆÙ„ÛŒØ¯ Ø³Ù†Ø§Ø±ÛŒÙˆ"):
    if not os.path.exists(INDEX_PATH):
        st.warning("Ø§Ø¨ØªØ¯Ø§ PDFÙ‡Ø§ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ/Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú©Ù†ÛŒØ¯.")
    elif not requirements or not description:
        st.warning("Ù„Ø·ÙØ§Ù‹ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ùˆ ØªÙˆØ¶ÛŒØ­ Ø³Ù†Ø§Ø±ÛŒÙˆ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    else:
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø³Ù†Ø§Ø±ÛŒÙˆ..."):
            embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            vectordb = FAISS.load_local(INDEX_PATH, embedder, allow_dangerous_deserialization=True)
            prompt = create_prompt(requirements, description, example_steps, example_desc)
            result = generate_test_scenario(prompt, vectordb)
        st.subheader("Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
        st.markdown(result.content)
